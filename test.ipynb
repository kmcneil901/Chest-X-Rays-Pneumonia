{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import models, optimizers, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "# from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'data/chest_xray/test/'\n",
    "train_path = 'data/chest_xray/train'\n",
    "val_path = 'data/chest_xray/val/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 624 images belonging to 2 classes.\n",
      "Found 867 images belonging to 1 classes.\n",
      "Found 16 images belonging to 2 classes.\n",
      "Found 867 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "#Ensuring that it using my gpu and loading data using my gpu\n",
    "with tf.device('/GPU:0'):\n",
    "\n",
    "    test_generator = ImageDataGenerator(\n",
    "        rescale=1.0/255).flow_from_directory(test_path, target_size=(64, 64), batch_size=312)\n",
    "    train_generator = ImageDataGenerator(\n",
    "        rescale=1.0/255).flow_from_directory(train_path, target_size=(64, 64), batch_size=2608)\n",
    "    val_generator = ImageDataGenerator(\n",
    "        rescale=1.0/255).flow_from_directory(val_path, target_size=(64, 64), batch_size=16)\n",
    "    aug_train_generator = ImageDataGenerator(rescale=1.0/255,\n",
    "                                             horizontal_flip=True,\n",
    "                                             height_shift_range=0.2,\n",
    "                                             width_shift_range=0.2,\n",
    "                                             rotation_range=45,\n",
    "                                             brightness_range=[0.2, 1],\n",
    "                                             zoom_range=0.2\n",
    "                                             ).flow_from_directory(train_path, target_size=(64, 64), batch_size=5000)\n",
    "\n",
    "    #Grabbing data\n",
    "    train_images, train_labels = next(train_generator)\n",
    "    test_images, test_labels = next(test_generator)\n",
    "    val_images, val_labels = next(val_generator)\n",
    "    aug_train_images, aug_train_labels = next(aug_train_generator)\n",
    "\n",
    "     # Create placeholders for train images\n",
    "    train_images = tf.Variable(initial_value=train_images, shape=train_images.shape, trainable=False, dtype=tf.float32)\n",
    "    test_images = tf.Variable(initial_value=test_images, shape=test_images.shape, trainable=False, dtype=tf.float32)\n",
    "    val_images = tf.Variable(initial_value=val_images, shape=val_images.shape, trainable=False, dtype=tf.float32)\n",
    "    aug_train_images = tf.Variable(initial_value=aug_train_images, shape=aug_train_images.shape, trainable=False, dtype=tf.float32)\n",
    "\n",
    "    #Creating labels\n",
    "    train_labels = tf.Variable(train_labels.T[0], trainable=False, dtype=tf.float32)\n",
    "    test_labels = tf.Variable(test_labels.T[0], trainable=False, dtype=tf.float32)\n",
    "    val_labels = tf.Variable(val_labels.T[0], trainable=False, dtype=tf.float32)\n",
    "    aug_train_labels = tf.Variable(aug_train_labels.T[0], trainable=False, dtype=tf.float32)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_images[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    train_loss, train_acc = model.evaluate(train_images.numpy(), train_labels.numpy())\n",
    "    test_loss, test_acc = model.evaluate(test_images.numpy(), test_labels.numpy())\n",
    "    printout = pd.DataFrame({'Dataset' : pd.Series(dtype='str'), \n",
    "            'accuracy' : pd.Series(dtype='float64'), \n",
    "            'log_loss': pd.Series(dtype='float64')})\n",
    "    test = ['Test', test_acc, test_loss]\n",
    "    train = ['Train', train_acc, train_loss]\n",
    "    printout.loc[len(printout.index)] = train\n",
    "    printout.loc[len(printout.index)] = test\n",
    "    return printout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try with only flat/dense\n",
    "#FSM\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32,\n",
    "                        kernel_size=(3, 3),\n",
    "                        activation='sigmoid',\n",
    "                        input_shape=(200, 200, 3)))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsm=model.fit(train_images.numpy(), train_labels.numpy(), initial_epoch=0, epochs = 25, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = models.Sequential()\n",
    "\n",
    "model2.add(Conv2D(filters=32,\n",
    "                        kernel_size=(3, 3),\n",
    "                        activation='sigmoid',\n",
    "                        input_shape=(200, 200, 3)))\n",
    "\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='sigmoid'))\n",
    "\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.1, 0.01, 0.05]\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    for lr in learning_rates:\n",
    "        #Setting learning rate\n",
    "        optimizer = optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "        #compilew new model\n",
    "        model2.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "        #fitting model\n",
    "        model2.fit(train_images.numpy(), train_labels.numpy(), epochs=25, batch_size=64,\n",
    "                   verbose=0)\n",
    "\n",
    "        #evaluate models\n",
    "        train_loss, train_accuracy = model2.evaluate(train_images.numpy(), train_labels.numpy(), batch_size = 64)\n",
    "        test_loss, test_accuracy = model2.evaluate(test_images.numpy(), test_labels.numpy(), batch_size = 64)\n",
    "\n",
    "        #printing an update\n",
    "        print(f\"Learning Rate: {lr}, Train Loss: {train_loss}, Train Accuracy: {train_accuracy}, Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a early stopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='loss',\n",
    "                               min_delta=1e-4,\n",
    "                               verbose=1,\n",
    "                               patience=5,\n",
    "                               mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = models.Sequential()\n",
    "\n",
    "model3.add(Conv2D(filters=32,\n",
    "                        kernel_size=(3, 3),\n",
    "                        activation='sigmoid',\n",
    "                        input_shape=(200, 200, 3)))\n",
    "model3.add(Dropout(.3))\n",
    "\n",
    "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model3.add(Conv2D(64, (3, 3), activation='sigmoid'))\n",
    "model3.add(Dropout(.3))\n",
    "\n",
    "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(12, activation='sigmoid'))\n",
    "# model3.add(Dense(6, activation='sigmoid'))\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model3.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(learning_rate=.1)\n",
    "model3.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model3.fit(train_images.numpy(), train_labels.numpy(), epochs=25, batch_size=64, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createmodel(function_name='sigmoid', filters=32, dense=1, \n",
    "                regularization_type=None, regularization_strength=0.01):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    if regularization_type == 'l1':\n",
    "        model.add(Conv2D(filters=filters, kernel_size=(3, 3), activation=function_name, kernel_regularizer=regularizers.l1(regularization_strength)))\n",
    "    elif regularization_type == 'l2':\n",
    "        model.add(Conv2D(filters=filters, kernel_size=(3, 3), activation=function_name, kernel_regularizer=regularizers.l2(regularization_strength)))\n",
    "    else:\n",
    "        model.add(Conv2D(filters=filters, kernel_size=(\n",
    "                      3, 3), activation=function_name))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Dropout(.3))\n",
    "    model.add(Conv2D(filters=filters,\n",
    "                     kernel_size=(3, 3),\n",
    "                     activation=function_name))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Dropout(.3))\n",
    "    model.add(Conv2D(filters=filters,\n",
    "                     kernel_size=(3, 3),\n",
    "                     activation=function_name))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    if dense == 3:\n",
    "        model.add(Dense(12, activation = 'sigmoid'))\n",
    "        model.add(Dense(6, activation='sigmoid'))\n",
    "    elif dense == 2:\n",
    "        model.add(Dense(6, activation = 'sigmoid'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = createmodel('tanh', 64)\n",
    "model4.fit(train_images.numpy(), train_labels.numpy(), epochs=25, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = createmodel('relu', 128, 3)\n",
    "model5.fit(train_images.numpy(), train_labels.numpy(), epochs=25, batch_size=64)\n",
    "evaluate(model5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = createmodel('relu', 128, 2)\n",
    "model6.fit(train_images.numpy(), train_labels.numpy(), epochs=25, batch_size=64)\n",
    "evaluate(model6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7 = models.Sequential()\n",
    "\n",
    "model7.add(Conv2D(filters=32,\n",
    "                        kernel_size=(3, 3),\n",
    "                        activation='sigmoid',\n",
    "                        input_shape=(200, 200, 3)))\n",
    "model7.add(Dropout(.3))\n",
    "\n",
    "model7.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model7.add(Conv2D(64, (3, 3), activation='sigmoid'))\n",
    "model7.add(Dropout(.3))\n",
    "\n",
    "model7.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model7.add(Flatten())\n",
    "model7.add(Dense(128, activation='sigmoid'))\n",
    "model7.add(Dropout(.3))\n",
    "model7.add(Dense(64, activation='sigmoid'))\n",
    "model7.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model7.summary()\n",
    "\n",
    "model7.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    model7.fit(train_images.numpy(), train_labels.numpy(), epochs=25, batch_size=64)\n",
    "evaluate(model7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8 = models.Sequential()\n",
    "\n",
    "model8.add(Conv2D(filters=32,\n",
    "                        kernel_size=(3, 3),\n",
    "                        activation='tanh',\n",
    "                        input_shape=(200, 200, 3)))\n",
    "model8.add(Dropout(.3))\n",
    "\n",
    "model8.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model8.add(Conv2D(64, (3, 3), activation='tanh'))\n",
    "model8.add(Dropout(.3))\n",
    "\n",
    "model8.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model8.add(Flatten())\n",
    "model8.add(Dense(128, activation='tanh'))\n",
    "model8.add(Dropout(.3))\n",
    "model8.add(Dense(64, activation='tanh'))\n",
    "model8.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model8.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    model8.fit(train_images.numpy(), train_labels.numpy(),\n",
    "               epochs=25, batch_size=64)\n",
    "    evaluate(model8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_model = models.Sequential()\n",
    "aug_model.add(Conv2D(\n",
    "    64, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "aug_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "aug_model.add(BatchNormalization())\n",
    "aug_model.add(Dropout(0.8))\n",
    "\n",
    "aug_model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "aug_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "aug_model.add(BatchNormalization())\n",
    "\n",
    "aug_model.add(Flatten())\n",
    "\n",
    "aug_model.add(Dense(256, activation='relu'))\n",
    "aug_model.add(BatchNormalization())\n",
    "\n",
    "aug_model.add(Dense(1, activation='sigmoid',\n",
    "                kernel_regularizer=regularizers.l2(0.001)))\n",
    "\n",
    "aug_model.compile(optimizer=optimizers.Adam(learning_rate=.001),\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy', Recall(), Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_model.fit(\n",
    "    aug_train_images, aug_train_labels, epochs=25, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc, train_recall, train_precision = aug_model_8.evaluate(\n",
    "    aug_train_images, aug_train_labels_final)\n",
    "test_loss, test_acc, test_recall, test_precision = aug_model_8.evaluate(\n",
    "    test_images, test_labels_final)\n",
    "printout = pd.DataFrame({'Dataset': pd.Series(dtype='str'),\n",
    "                         'accuracy': pd.Series(dtype='float64'),\n",
    "                         'log_loss': pd.Series(dtype='float64'),\n",
    "                         'precision': pd.Series(dtype='float64'),\n",
    "                         'recall': pd.Series(dtype='float64')})\n",
    "test = ['Test', test_acc, test_loss, test_recall, test_precision]\n",
    "train = ['Train', train_acc, train_loss, train_recall, train_precision]\n",
    "printout.loc[len(printout.index)] = train\n",
    "printout.loc[len(printout.index)] = test\n",
    "printout\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
